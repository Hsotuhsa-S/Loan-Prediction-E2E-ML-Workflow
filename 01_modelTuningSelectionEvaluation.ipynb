{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# Loan Approval Classification: Model Tuning, Selection & Evaluation\n",
    "\n",
    "**Project:** Data Science/ML Practice  \n",
    "**Objective:** Compare, tune, and evaluate two tree-based classification models (Random Forest and Gradient Boosting) for loan approval prediction. Save the best-performing model for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-load-data",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "Load the cleaned data from Notebook 1. Identify numeric and categorical features. Set up the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: ['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']\n",
      "Numeric features: ['person_age', 'person_income', 'person_emp_exp', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# Load cleaned data\n",
    "data = pd.read_csv('data/loanApproval/cleanLoanApprovalData.csv')\n",
    "\n",
    "# Set target and features\n",
    "target_col = 'loan_status'\n",
    "X = data.drop(columns=target_col)\n",
    "y = data[target_col].astype(int)  # Convert bool to int for sklearn compatibility\n",
    "\n",
    "# Identify feature types\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numeric_features = X.select_dtypes(include=[np.number, 'bool']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "print(f\"Numeric features: {numeric_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-split",
   "metadata": {},
   "source": [
    "## 2. Train/Test Split & Cross-Validation Strategy\n",
    "\n",
    "### Train/Test Split (80/20)\n",
    "- **Purpose**: Keep 20% of data completely unseen during training to get honest performance estimates\n",
    "- **Stratified Split**: Maintains the same proportion of approved/rejected loans in both train and test sets\n",
    "\n",
    "### Cross-Validation Strategy\n",
    "- **5-Fold Stratified CV**: Splits training data into 5 parts, trains on 4, validates on 1, repeats 5 times\n",
    "- **Stratified**: Each fold maintains the original class distribution (loan approval ratios)\n",
    "\n",
    "**Correct Flow**: \n",
    "1. **Split**: Full Data → Train Data (80%) + Test Data (20%)\n",
    "2. **Tune**: Train Data → GridSearchCV with 5-Fold CV → Best Parameters\n",
    "3. **Train**: Best Parameters + **Full Train Data** → Final Model \n",
    "4. **Evaluate**: Final Model → Test Data → Final Performance Metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "split-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 36000, Test size: 9000\n"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(f\"Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}\")\n",
    "\n",
    "# Stratified cross-validation for imbalanced data\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-encoding",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Pipeline\n",
    "One-hot encode categoricals. No scaling required for tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "preprocessing-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for tree models: only one-hot encoding for categoricals\n",
    "categorical_transformer = Pipeline([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-model-pipeline",
   "metadata": {},
   "source": [
    "## 4. Model Pipelines and Hyperparameter Grids\n",
    "Define comprehensive hyperparameter grids based on data characteristics and model.\n",
    "\n",
    "### Hyperparameter Strategy:\n",
    "- **Random Forest**: Focus on tree depth, feature sampling, and ensemble size\n",
    "- **Gradient Boosting**: Tune learning rate, regularization, and tree complexity\n",
    "- **Data-Driven Ranges**: Based on feature count (~{len(X.columns)}) and sample size (~{len(X)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "model-pipelines",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 45000 samples, 13 features\n",
      "Class distribution: {0: 35000, 1: 10000}\n",
      "min_samples_split range: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter grids based on data characteristics\n",
    "n_features = X.shape[1]\n",
    "n_samples = X.shape[0]\n",
    "\n",
    "print(f\"Dataset: {n_samples} samples, {n_features} features\")\n",
    "print(f\"Class distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Simple feature-driven min_samples_split range (2 to n_features-1)\n",
    "min_samples_split_range = list(range(2, n_features))\n",
    "print(f\"min_samples_split range: {min_samples_split_range}\")\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": {\n",
    "        \"estimator\": Pipeline([\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1))\n",
    "        ]),\n",
    "        # Comprehensive grid based on data size and features\n",
    "        \"param_grid\": {\n",
    "            \"classifier__n_estimators\": [100, 200, 300],  # More trees for better performance\n",
    "           # \"classifier__max_depth\": [10, 15, 20, None],  # Control overfitting\n",
    "            \"classifier__max_features\": [\"sqrt\", \"log2\", 0.3],  # Feature sampling strategies\n",
    "           # \"classifier__min_samples_split\": min_samples_split_range,  # 2 to n_features-1\n",
    "            \"classifier__min_samples_split\": [2, 3, 5, 8],\n",
    "           # \"classifier__min_samples_leaf\": [1, 2, 4],    # Leaf node regularization\n",
    "        },\n",
    "        \"results\": {}\n",
    "    },\n",
    "    \"GradientBoosting\": {\n",
    "        \"estimator\": Pipeline([\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", GradientBoostingClassifier(random_state=42))\n",
    "        ]),\n",
    "        # Learning rate and regularization focused\n",
    "        \"param_grid\": {\n",
    "            \"classifier__n_estimators\": [100, 200, 300],\n",
    "           # \"classifier__learning_rate\": [0.05, 0.1, 0.15],  # Critical for GB performance\n",
    "           # \"classifier__max_depth\": [3, 5, 7],              # Shallow trees for GB\n",
    "            \"classifier__max_features\": [\"sqrt\", \"log2\"],\n",
    "            #\"classifier__min_samples_split\": min_samples_split_range,  # 2 to n_features-1\n",
    "            \"classifier__min_samples_split\": [2, 3, 5, 8],\n",
    "           # \"classifier__subsample\": [0.8, 0.9, 1.0],        # Stochastic gradient boosting\n",
    "        },\n",
    "        \"results\": {}\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-train-eval-fn",
   "metadata": {},
   "source": [
    "## 5. Model Training, Tuning, Evaluation\n",
    "Train models with cross-validation, tune parameters, and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "train-eval-fn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model_name, model_dict, X_train, X_test, y_train, y_test, categorical_features, cv):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model, storing metrics and best params.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str\n",
    "        Name of the model for display purposes\n",
    "    model_dict : dict\n",
    "        Dictionary containing model estimator and parameter grid\n",
    "    X_train, X_test : pandas.DataFrame\n",
    "        Training and test feature sets\n",
    "    y_train, y_test : pandas.Series\n",
    "        Training and test target variables\n",
    "    categorical_features : list\n",
    "        List of categorical feature column names\n",
    "    cv : sklearn cross-validation object\n",
    "        Cross-validation strategy (e.g., StratifiedKFold)\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Updated model dictionary with results\n",
    "    \n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError: If model training fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\n==== {model_name} ====\")\n",
    "        grid = GridSearchCV(\n",
    "            model_dict[\"estimator\"], \n",
    "            model_dict[\"param_grid\"], \n",
    "            cv=cv, \n",
    "            scoring=\"roc_auc\", \n",
    "            n_jobs=-1, \n",
    "            return_train_score=True,\n",
    "            verbose=1 # Add progress tracking\n",
    "        )\n",
    "        \n",
    "        grid.fit(X_train, y_train)\n",
    "        \n",
    "        # Store grid and best results\n",
    "        model_dict[\"grid\"] = grid\n",
    "        model_dict[\"results\"][\"best_params\"] = grid.best_params_\n",
    "        model_dict[\"results\"][\"best_cv_auc\"] = grid.best_score_\n",
    "\n",
    "        print(\"Best params:\", grid.best_params_)\n",
    "        print(\"Best CV ROC-AUC:\", grid.best_score_)\n",
    "        \n",
    "        # Test set evaluation\n",
    "        try:\n",
    "            y_pred = grid.predict(X_test)\n",
    "            y_proba = grid.predict_proba(X_test)[:,1]\n",
    "\n",
    "            model_dict[\"results\"][\"test_auc\"] = roc_auc_score(y_test, y_proba)\n",
    "            model_dict[\"results\"][\"test_accuracy\"] = accuracy_score(y_test, y_pred)\n",
    "            model_dict[\"results\"][\"test_precision\"] = precision_score(y_test, y_pred)\n",
    "            model_dict[\"results\"][\"test_recall\"] = recall_score(y_test, y_pred)\n",
    "            model_dict[\"results\"][\"test_f1\"] = f1_score(y_test, y_pred)\n",
    "            model_dict[\"results\"][\"confusion_matrix\"] = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "            # print(\"Test set AUC:\", model_dict[\"results\"][\"test_auc\"])\n",
    "            print(f\"✓ Test AUC: {model_dict['results']['test_auc']:.4f}\")\n",
    "\n",
    "            # Cross-validated AUCs for visualization\n",
    "            model_dict[\"results\"][\"cv_aucs\"] = cross_val_score(grid.best_estimator_, X_train, y_train, cv=cv, scoring=\"roc_auc\")\n",
    "            print(\"Cross-validated AUCs:\", model_dict[\"results\"][\"cv_aucs\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error in test evaluation: {str(e)}\")\n",
    "            raise ValueError(f\"Test evaluation failed: {str(e)}\")    \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error training {model_name}: {str(e)}\")\n",
    "        model_dict[\"results\"][\"error\"] = str(e)\n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-train-models",
   "metadata": {},
   "source": [
    "## 6. Train and Evaluate Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-models",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== RandomForest ====\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best params: {'classifier__max_features': 0.3, 'classifier__min_samples_split': 3, 'classifier__n_estimators': 300}\n",
      "Best CV ROC-AUC: 0.974212310267857\n",
      "✓ Test AUC: 0.9761\n",
      "Cross-validated AUCs: [0.97384805 0.97091451 0.97670179 0.97378359 0.97581367]\n",
      "\n",
      "==== GradientBoosting ====\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best params: {'classifier__max_features': 'sqrt', 'classifier__min_samples_split': 3, 'classifier__n_estimators': 300}\n",
      "Best CV ROC-AUC: 0.9733897098214286\n",
      "✓ Test AUC: 0.9745\n"
     ]
    }
   ],
   "source": [
    "for name in models:\n",
    "    train_and_evaluate(\n",
    "        name,\n",
    "        models[name],\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        categorical_features,\n",
    "        cv  # Pass the cv object as parameter\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7-eval-visual",
   "metadata": {},
   "source": [
    "## 7. Model Comparison: Visualize ROC and CV AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves for both models\n",
    "plt.figure(figsize=(8,6))\n",
    "for name, model in models.items():\n",
    "    grid = model[\"grid\"]\n",
    "    y_proba = grid.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC: {auc:.2f})\")\n",
    "plt.plot([0,1],[0,1],'--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Boxplot of CV AUCs\n",
    "cv_auc_dict = {name: model[\"results\"][\"cv_aucs\"] for name, model in models.items() if \"cv_aucs\" in model[\"results\"]}\n",
    "cv_auc_df = pd.DataFrame(cv_auc_dict)\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.boxplot(data=cv_auc_df)\n",
    "plt.ylabel(\"ROC-AUC (CV)\")\n",
    "plt.title(\"Model ROC-AUC Resampling Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display CV metric stability (standard deviation)\n",
    "for name, model in models.items():\n",
    "    if \"results\" in model and \"cv_aucs\" in model[\"results\"]:\n",
    "        cv_aucs = model[\"results\"][\"cv_aucs\"]\n",
    "        cv_std = np.std(cv_aucs)\n",
    "        cv_mean = np.mean(cv_aucs)\n",
    "\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Cross-Validation Stability: {name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"CV ROC-AUC: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "        print(f\"CV Standard Deviation: {cv_std:.4f}\")\n",
    "                    \n",
    "        if cv_std < 0.02:\n",
    "            print(\"✓ Model is stable (CV std < 0.02)\")\n",
    "        else:\n",
    "            print(\"! Model stability could be improved (CV std ≥ 0.02)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beabf3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model_dict: dict, feature_names: list, top_n: int = 15):\n",
    "    \"\"\"\n",
    "    Analyze and visualize feature importance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_dict : dict\n",
    "        Trained model dictionary\n",
    "    feature_names : list\n",
    "        List of all feature names\n",
    "    top_n : int\n",
    "        Number of top features to display\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if \"grid\" not in model_dict or model_dict[\"grid\"] is None:\n",
    "            print(\"No trained model found\")\n",
    "            return\n",
    "        \n",
    "        classifier = model_dict[\"grid\"].best_estimator_.named_steps['classifier']\n",
    "        \n",
    "        if hasattr(classifier, 'feature_importances_'):\n",
    "            importances = classifier.feature_importances_\n",
    "            \n",
    "            # Create importance DataFrame\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': importances\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            # Store results\n",
    "            model_dict[\"results\"][\"feature_importance\"] = importance_df\n",
    "            \n",
    "            # Visualization\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            top_features = importance_df.head(top_n)\n",
    "            sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')\n",
    "            plt.title(f'Top {top_n} Feature Importances')\n",
    "            plt.xlabel('Importance Score')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        # Extract and display normalized feature importance as percentages\n",
    "        # print(f\"Top {min(top_n, len(importance_df))} Features:\")\n",
    "        # print(importance_df.head(top_n))\n",
    "\n",
    "        if \"results\" in models[model_name] and \"feature_importance\" in models[model_name][\"results\"]:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Normalized Top {top_n} Feature Importance (Percentages): {model_name}\")\n",
    "            print(f\"{'='*50}\")\n",
    "        \n",
    "            # Get feature importance and normalize to percentages\n",
    "            importance_df = models[model_name][\"results\"][\"feature_importance\"].copy()\n",
    "            importance_df['importance_pct'] = importance_df['importance'] * 100\n",
    "            \n",
    "            # Display top 10 features with percentage values\n",
    "            top_features = importance_df.head(top_n)\n",
    "            print(top_features[['feature', 'importance', 'importance_pct']])\n",
    "            print(\"\\nTop 3 predictors:\")\n",
    "            for i, row in top_features.head(3).iterrows():\n",
    "                print(f\"- {row['feature']}: {row['importance']:.2f} ({row['importance_pct']:.1f}%)\")\n",
    "\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing feature importance: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa3e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after preprocessing\n",
    "# Extract categorical feature names from one-hot encoder\n",
    "ohe_features = models[name][\"grid\"].best_estimator_.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "all_feature_names = list(ohe_features) + numeric_features\n",
    "\n",
    "# Analyze feature importance for both models\n",
    "# Extract and display normalized feature importance as percentages\n",
    "for model_name in models:\n",
    "\n",
    "    if \"grid\" in models[model_name] and models[model_name][\"grid\"] is not None:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Feature Importance Analysis: {model_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        analyze_feature_importance(models[model_name], all_feature_names, top_n=10)\n",
    "# Save the best model (Random Forest) for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8-summary-metrics",
   "metadata": {},
   "source": [
    "## 8. Summary Table: Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for name, model in models.items():\n",
    "    r = model[\"results\"]\n",
    "    metrics.append({\n",
    "        \"Model\": name,\n",
    "        \"Test AUC\": r[\"test_auc\"],\n",
    "        \"Accuracy\": r[\"test_accuracy\"],\n",
    "        \"Precision\": r[\"test_precision\"],\n",
    "        \"Recall\": r[\"test_recall\"],\n",
    "        \"F1\": r[\"test_f1\"]\n",
    "    })\n",
    "summary = pd.DataFrame(metrics)\n",
    "print(\"\\nModel Performance Metrics:\\n\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step9-save-best-model",
   "metadata": {},
   "source": [
    "## 9. Save the Best Model\n",
    "Select and save the best-performing model (highest ROC-AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model by test ROC-AUC\n",
    "best_model_name = summary.sort_values(by='Test AUC', ascending=False).iloc[0]['Model']\n",
    "best_pipeline = models[best_model_name][\"grid\"].best_estimator_\n",
    "model_path = f'model/loanApproval/Loan_{best_model_name}_model.pkl'\n",
    "joblib.dump(best_pipeline, model_path)\n",
    "print(f\"Best model ({best_model_name}) saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step10-conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- Both Random Forest and Gradient Boosting models were trained, tuned and evaluated.\n",
    "- The best-performing model Random Forest(by ROC-AUC) was saved for deployment.\n",
    "- Feature importance and evaluation metrics can be reviewed above.\n",
    "\n",
    "### Next Steps\n",
    "- Deploy the saved model to Hugging Face or another serving platform as needed.\n",
    "- Use modular code and templates for future ML projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "nbformat": 4,
  "nbformat_minor": 5
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
